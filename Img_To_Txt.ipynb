{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17Z-AX0a9J5PmOJiWqweQp90c-ZMkP8dg",
      "authorship_tag": "ABX9TyN0iOrgsK8jdFeWiu/pjhLx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheryar-bit/Lang-Chain/blob/main/Img_To_Txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch pillow"
      ],
      "metadata": {
        "id": "zI-sO79yNO26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCPAPOXsI8aA"
      },
      "outputs": [],
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loafing model and transformer needed( Using bit slow models for now  )"
      ],
      "metadata": {
        "id": "6Z4bjIJ7RzSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Check if GPU available or not if not then use CPU\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "WSZgxQSCJEgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 16\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
      ],
      "metadata": {
        "id": "T26HK_mwL3pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create captioning function in Python"
      ],
      "metadata": {
        "id": "PC9-8cDRR_WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#takes in a list of image file paths and generates text captions for them using a vision-language model.\n",
        "def predict_step(image_paths):\n",
        "  images = []\n",
        "\n",
        "  ## will Loop over each image path provided\n",
        "  for image_path in image_paths:\n",
        "    i_image = Image.open(image_path)\n",
        "    #Optional (RGB color)\n",
        "    if i_image.mode != \"RGB\":\n",
        "      i_image = i_image.convert(mode=\"RGB\")\n",
        "\n",
        "    #Add the processed image to the list\n",
        "    images.append(i_image)\n",
        "\n",
        "  # This block takes the preprocessed image tensors, feeds them into the vision-language model to\n",
        "  # generate captions, decodes the output token IDs into human-readable text, cleans up whitespace,\n",
        "\n",
        "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values  # Extracting pixel values (numerical tensors) from images using the feature extractor\n",
        "  pixel_values = pixel_values.to(device)\n",
        "\n",
        "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "\n",
        "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "  #  Strip leading/trailing whitespace from each prediction\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  return preds"
      ],
      "metadata": {
        "id": "Eo-y9kCgMBsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('') ##Any Image text will ge genrated"
      ],
      "metadata": {
        "id": "J3RcS_gQPK-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}